import json
import re
import operator
import os
from typing import TypedDict, Annotated, List
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from .prompts import expert_prompt, novice_prompt

# ğŸ‘‡ 1. å¼•å…¥ dotenv åº“
from dotenv import load_dotenv

# ğŸ‘‡ 2. åŠ è½½ .env æ–‡ä»¶ (å®ƒä¼šè‡ªåŠ¨æ‰¾ OPENAI_API_KEY å’Œ OPENAI_API_BASE)
load_dotenv()

# =======================================================
# ğŸ›¡ï¸ é…ç½® LLM (æ™ºè°± OpenAI å…¼å®¹æ¨¡å¼)
# =======================================================

# ä»ç¯å¢ƒå˜é‡è¯»å– (å¯¹åº”ä½  .env é‡Œçš„åå­—)
api_key = os.getenv("OPENAI_API_KEY")
api_base = os.getenv("OPENAI_API_BASE", "https://open.bigmodel.cn/api/paas/v4/") # é»˜è®¤å€¼é˜²å‘†

if not api_key:
    raise ValueError("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° OPENAI_API_KEYï¼è¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")

# åˆå§‹åŒ– LangChain çš„ OpenAI å®¢æˆ·ç«¯
# è™½ç„¶åå­—å« ChatOpenAIï¼Œä½†æŒ‡å‘äº†æ™ºè°±çš„æœåŠ¡å™¨
llm = ChatOpenAI(
    model="glm-4",           # æ™ºè°±çš„æ¨¡å‹åç§° (ä¹Ÿå¯ä»¥æ”¹ glm-4-flash, glm-4-plus)
    temperature=0.1,         # æ¸©åº¦ä½ä¸€ç‚¹ï¼Œè®©ä¸“å®¶æ›´ä¸¥è°¨
    openai_api_key=api_key,  # ä¼ å…¥æ™ºè°± Key
    openai_api_base=api_base # ä¼ å…¥æ™ºè°± URL
)

# =======================================================
# ğŸ§  ä¸‹é¢æ˜¯æ ¸å¿ƒé€»è¾‘ (ä¸éœ€è¦æ”¹åŠ¨)
# =======================================================

def parse_json_robust(text: str):
    text = text.strip()
    try:
        return json.loads(text)
    except:
        pass
    if "```" in text:
        pattern = r"```(?:json)?\s*(.*?)\s*```"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            text = match.group(1)
    match = re.search(r"\{.*\}", text, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            json_str = json_str.replace('\n', ' ')
            return json.loads(json_str)
        except:
            pass
    return None

class SimulationState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    domain: str
    taxonomy_context: str
    secret_mission: dict
    is_concluded: bool
    turn_count: int

def expert_node(state: SimulationState):
    chain = expert_prompt | llm
    response = chain.invoke({
        "domain": state["domain"],
        "taxonomy_context": state["taxonomy_context"],
        "messages": state["messages"]
    })
    
    data = parse_json_robust(response.content)
    
    if data:
        question = data.get("question", str(data))
        is_done = data.get("is_conclusion", False)
        # å®¹é”™å¤„ç†ï¼šæœ‰æ—¶ LLM ä¼šè¿”å›å­—ç¬¦ä¸² "true"
        if isinstance(is_done, str) and is_done.lower() == 'true':
            is_done = True
    else:
        question = response.content
        is_done = False
        
    return {
        "messages": [AIMessage(content=question)],
        "is_concluded": is_done,
        "turn_count": state["turn_count"] + 1
    }

def novice_node(state: SimulationState):
    if state["is_concluded"]:
        return {"messages": []}

    chain = novice_prompt | llm
    mission = state["secret_mission"]
    category = mission.get("category_name", mission.get("category", "é€šç”¨å’¨è¯¢"))

    response = chain.invoke({
        "secret_expert_term": mission["expert_term"],
        "secret_user_intent": mission["novice_intent"],
        "secret_category": category,
        "messages": state["messages"]
    })
    
    data = parse_json_robust(response.content)
    if data:
        reply = data.get("response", str(data))
    else:
        reply = response.content
        
    return {"messages": [HumanMessage(content=reply)]}

# --- ç»„è£…å·¥ä½œæµ ---
workflow = StateGraph(SimulationState)
workflow.add_node("expert", expert_node)
workflow.add_node("novice", novice_node)

workflow.set_entry_point("expert")
workflow.add_edge("expert", "novice")
workflow.add_edge("novice", END)

app = workflow.compile()