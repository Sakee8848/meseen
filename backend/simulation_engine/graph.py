"""
ğŸ”„ Meseeing å¯†å¿ƒ - LangGraph å¤šè½®åšå¼ˆå·¥ä½œæµ V6.0
=================================================
æ ¸å¿ƒæ”¹å˜ï¼š
1. å®ç°çœŸæ­£çš„å¤šè½®å¾ªç¯ï¼ˆæœ€å¤š 10 è½®ï¼‰
2. ä¸“å®¶å¿…é¡»è¿½é—®è‡³å°‘ 2-3 æ¬¡æ‰èƒ½å¾—å‡ºç»“è®º
3. è®°å½•å®Œæ•´çš„è¯Šæ–­æ¨ç†é“¾
4. è®¡ç®—æ¯è½®çš„ä¿¡æ¯å¢ç›Š
"""

import json
import re
import operator
import os
from typing import TypedDict, Annotated, List, Optional
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from .prompts import expert_prompt, novice_prompt, opening_prompt
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# =======================================================
# ğŸ›¡ï¸ é…ç½® LLM
# =======================================================
# =======================================================
# ğŸ›¡ï¸ é…ç½® LLM (æ”¯æŒè‡ªåŠ¨æ•…éšœåˆ‡æ¢)
# =======================================================
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

# 1. åˆå§‹åŒ– Google LLM (é¦–é€‰)
google_llm = None
google_api_key = os.getenv("GOOGLE_API_KEY")
if google_api_key:
    try:
        google_llm = ChatGoogleGenerativeAI(
            model="gemini-flash-latest",
            temperature=0.3,
            google_api_key=google_api_key,
            convert_system_message_to_human=True,
            transport="rest"
        )
        print("   âœ… Google Gemini é…ç½®æˆåŠŸ (é¦–é€‰)")
    except Exception as e:
        print(f"   âš ï¸ Google Gemini åˆå§‹åŒ–å¤±è´¥: {e}")

# 2. åˆå§‹åŒ– OpenAI/Zhipu LLM (å¤‡é€‰)
openai_llm = None
openai_api_key = os.getenv("OPENAI_API_KEY")
openai_api_base = os.getenv("OPENAI_API_BASE", "https://open.bigmodel.cn/api/paas/v4/")

if openai_api_key:
    try:
        openai_llm = ChatOpenAI(
            model="glm-4",
            temperature=0.3,
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base
        )
        print("   âœ… OpenAI/GLM-4 é…ç½®æˆåŠŸ (å¤‡é€‰)")
    except Exception as e:
        print(f"   âš ï¸ OpenAI/GLM-4 åˆå§‹åŒ–å¤±è´¥: {e}")

# 3. é…ç½®æœ€ç»ˆ LLM ä¸æ•…éšœåˆ‡æ¢ç­–ç•¥
llm = None
llm_provider = os.getenv("LLM_PROVIDER", "openai").lower()

if llm_provider == "google":
    if google_llm:
        if openai_llm:
            # å¯ç”¨è‡ªåŠ¨æ•…éšœåˆ‡æ¢: Google -> OpenAI
            llm = google_llm.with_fallbacks([openai_llm])
            print("   ğŸš€ ç­–ç•¥: ä¼˜å…ˆä½¿ç”¨ Google Geminiï¼Œå¤±è´¥è‡ªåŠ¨åˆ‡æ¢è‡³ OpenAI/GLM-4")
        else:
            llm = google_llm
            print("   ğŸ‘‰ ç­–ç•¥: ä»…ä½¿ç”¨ Google Gemini")
    else:
        # å¦‚æœæŒ‡å®š Google ä½†æ²¡é…ç½®å¥½ï¼Œå›é€€åˆ° OpenAI
        if openai_llm:
            print("   âš ï¸ è­¦å‘Š: Google æœªé…ç½®ï¼Œé™çº§ä½¿ç”¨ OpenAI/GLM-4")
            llm = openai_llm
        else:
            raise ValueError("âŒ é”™è¯¯ï¼šæœªé…ç½®ä»»ä½•æœ‰æ•ˆçš„ LLM API Keyï¼")
else:
    # é»˜è®¤ OpenAI
    if openai_llm:
        llm = openai_llm
        print("   ğŸ‘‰ ç­–ç•¥: ä»…ä½¿ç”¨ OpenAI/GLM-4")
    else:
         raise ValueError("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° OPENAI_API_KEYï¼")

# =======================================================
# ğŸ“Š çŠ¶æ€å®šä¹‰ (å¢å¼ºç‰ˆ)
# =======================================================
class SimulationState(TypedDict):
    """ä»¿çœŸçŠ¶æ€ - åŒ…å«å®Œæ•´çš„è¯Šæ–­è¿½è¸ª"""
    messages: Annotated[List[BaseMessage], operator.add]
    domain: str
    taxonomy_context: str
    secret_mission: dict  # å°ç™½çš„ç§˜å¯†ä»»åŠ¡
    is_concluded: bool
    turn_count: int
    max_turns: int  # æœ€å¤§è½®æ¬¡é™åˆ¶
    
    # ğŸ†• è¯Šæ–­è¿½è¸ªæ•°æ®
    diagnosis_trace: List[dict]  # æ¯è½®çš„è¯Šæ–­æ¨ç†è®°å½•
    key_questions: List[dict]  # å…³é”®è¿½é—®åˆ—è¡¨
    eliminated_categories: List[str]  # å·²æ’é™¤çš„åˆ†ç±»
    confidence_history: List[float]  # ç½®ä¿¡åº¦å˜åŒ–æ›²çº¿
    final_diagnosis: Optional[dict]  # æœ€ç»ˆè¯Šæ–­ç»“æœ


# =======================================================
# ğŸ­ ç”Ÿæˆå¼€åœºç™½èŠ‚ç‚¹
# =======================================================
def generate_opening_node(state: SimulationState) -> dict:
    """ç”Ÿæˆå°ç™½çš„å¼€åœºç™½ï¼ˆç¡®ä¿ä¸æ³„éœ²ç­”æ¡ˆï¼‰"""
    mission = state["secret_mission"]
    
    chain = opening_prompt | llm
    response = chain.invoke({
        "secret_user_intent": mission.get("novice_intent", ""),
        "secret_category": mission.get("category", ""),
        "persona_role": mission.get("persona", "æ™®é€šäºº"),
        "persona_tone": mission.get("tone", "ç„¦è™‘")
    })
    
    opening = response.content.strip()
    # å»æ‰å¯èƒ½çš„å¼•å·
    opening = opening.strip('"\'')
    
    print(f"   ğŸ’¬ å°ç™½å¼€åœº: {opening[:50]}...")
    
    return {
        "messages": [HumanMessage(content=opening)],
        "turn_count": 1
    }


# =======================================================
# ğŸ¤– ä¸“å®¶è¯Šæ–­èŠ‚ç‚¹
# =======================================================
def expert_node(state: SimulationState) -> dict:
    """ä¸“å®¶è¿›è¡Œè¯Šæ–­è¿½é—®"""
    chain = expert_prompt | llm
    
    response = chain.invoke({
        "domain": state["domain"],
        "taxonomy_context": state["taxonomy_context"],
        "messages": state["messages"]
    })
    
    data = parse_json_robust(response.content)
    
    # æå–è¯Šæ–­æ•°æ®
    diagnosis_trace_entry = {}
    reply = ""
    is_done = False
    confidence = 0.0
    
    if data:
        # æå–è¯Šæ–­æ¨ç†
        reasoning = data.get("diagnosis_reasoning", {})
        analysis = data.get("analysis_data", {})
        reply = data.get("reply_to_user", str(data))
        
        diagnosis_trace_entry = {
            "turn": state["turn_count"],
            "hypotheses": reasoning.get("current_hypotheses", []),
            "key_signals": reasoning.get("key_signals", []),
            "question_purpose": reasoning.get("next_question_purpose", ""),
            "eliminated": reasoning.get("eliminated_categories", []),
            "confidence": reasoning.get("confidence", 0.5),
            "diagnosis": analysis.get("diagnosis", ""),
            "matched_service": analysis.get("matched_service", "")
        }
        
        confidence = reasoning.get("confidence", 0.5)
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        status = analysis.get("status", "active")
        is_done = (status == "concluded" and state["turn_count"] >= 3)  # è‡³å°‘ 3 è½®
        
    else:
        reply = response.content
        diagnosis_trace_entry = {
            "turn": state["turn_count"],
            "raw_response": reply[:200]
        }
    
    print(f"   ğŸ¤– ä¸“å®¶è¿½é—® (T{state['turn_count']}): {reply[:50]}...")
    
    # æ›´æ–°è¿½è¸ªæ•°æ®
    new_trace = state.get("diagnosis_trace", []) + [diagnosis_trace_entry]
    new_confidence = state.get("confidence_history", []) + [confidence]
    new_eliminated = state.get("eliminated_categories", []) + diagnosis_trace_entry.get("eliminated", [])
    
    # å¦‚æœç»“æŸï¼Œè®°å½•æœ€ç»ˆè¯Šæ–­
    final_diagnosis = None
    if is_done and data:
        final_diagnosis = {
            "service": data.get("analysis_data", {}).get("matched_service", ""),
            "diagnosis": data.get("analysis_data", {}).get("diagnosis", ""),
            "confidence": confidence,
            "total_turns": state["turn_count"],
            "key_questions": [t.get("question_purpose", "") for t in new_trace if t.get("question_purpose")]
        }
    
    return {
        "messages": [AIMessage(content=reply)],
        "is_concluded": is_done,
        "diagnosis_trace": new_trace,
        "confidence_history": new_confidence,
        "eliminated_categories": list(set(new_eliminated)),
        "final_diagnosis": final_diagnosis
    }


# =======================================================
# ğŸ‘¤ å°ç™½å›å¤èŠ‚ç‚¹
# =======================================================
def novice_node(state: SimulationState) -> dict:
    """å°ç™½æ ¹æ®ä¸“å®¶è¿½é—®è¿›è¡Œå›å¤"""
    if state["is_concluded"]:
        return {"messages": []}
    
    mission = state["secret_mission"]
    chain = novice_prompt | llm
    
    response = chain.invoke({
        "secret_user_intent": mission.get("novice_intent", ""),
        "secret_category": mission.get("category", ""),
        "persona_role": mission.get("persona", "æ™®é€šäºº"),
        "persona_tone": mission.get("tone", "ç„¦è™‘"),
        "messages": state["messages"]
    })
    
    data = parse_json_robust(response.content)
    
    if data:
        reply = data.get("response", str(data))
        # è®°å½•é€éœ²å’Œéšè—çš„ä¿¡æ¯
        revealed = data.get("revealed_info", [])
        hidden = data.get("hidden_info", [])
        print(f"   ğŸ‘¤ å°ç™½å›å¤: {reply[:50]}... (é€éœ²: {len(revealed)}, éšè—: {len(hidden)})")
    else:
        reply = response.content
        print(f"   ğŸ‘¤ å°ç™½å›å¤: {reply[:50]}...")
    
    return {
        "messages": [HumanMessage(content=reply)],
        "turn_count": state["turn_count"] + 1
    }


# =======================================================
# ğŸ”€ æ¡ä»¶åˆ¤æ–­å‡½æ•°
# =======================================================
def should_continue(state: SimulationState) -> str:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­å¯¹è¯"""
    # å¦‚æœå·²ç»ç»“æŸ
    if state.get("is_concluded", False):
        print(f"   âœ… è¯Šæ–­å®Œæˆ! æ€»è½®æ¬¡: {state['turn_count']}")
        return "end"
    
    # å¦‚æœè¶…è¿‡æœ€å¤§è½®æ¬¡
    max_turns = state.get("max_turns", 10)
    if state["turn_count"] >= max_turns:
        print(f"   âš ï¸ è¾¾åˆ°æœ€å¤§è½®æ¬¡ ({max_turns})ï¼Œå¼ºåˆ¶ç»“æŸ")
        return "end"
    
    # ç»§ç»­å¯¹è¯
    return "continue"


# =======================================================
# ğŸ”„ ç»„è£…å·¥ä½œæµ (å¤šè½®å¾ªç¯ç‰ˆ)
# =======================================================
workflow = StateGraph(SimulationState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("opening", generate_opening_node)
workflow.add_node("expert", expert_node)
workflow.add_node("novice", novice_node)

# è®¾ç½®å…¥å£ï¼šå…ˆç”Ÿæˆå¼€åœºç™½
workflow.set_entry_point("opening")

# å¼€åœºç™½åè¿›å…¥ä¸“å®¶è¯Šæ–­
workflow.add_edge("opening", "expert")

# ä¸“å®¶è¯Šæ–­åè¿›å…¥å°ç™½å›å¤
workflow.add_edge("expert", "novice")

# å°ç™½å›å¤åï¼Œæ¡ä»¶åˆ¤æ–­æ˜¯å¦ç»§ç»­
workflow.add_conditional_edges(
    "novice",
    should_continue,
    {
        "continue": "expert",  # ç»§ç»­ä¸‹ä¸€è½®è¿½é—®
        "end": END            # ç»“æŸå¯¹è¯
    }
)

# ç¼–è¯‘å·¥ä½œæµ
app = workflow.compile()


# =======================================================
# ğŸ§ª æµ‹è¯•å‡½æ•°
# =======================================================
def run_simulation_test():
    """è¿è¡Œä¸€æ¬¡å®Œæ•´çš„ä»¿çœŸæµ‹è¯•"""
    from .domain_manager import DomainManager
    
    dm = DomainManager("hr")
    mission = dm.generate_secret_mission()
    
    print("=" * 60)
    print("ğŸ­ å¼€å§‹ AI äº’åšä»¿çœŸ")
    print("=" * 60)
    print(f"ğŸ“‹ ç§˜å¯†ä»»åŠ¡: {mission['novice_intent'][:50]}...")
    print(f"ğŸ¯ ç›®æ ‡åˆ†ç±»: {mission['expert_term']}")
    print(f"ğŸ‘¤ è§’è‰²: {mission.get('persona', 'N/A')} ({mission.get('tone', 'N/A')})")
    print("-" * 60)
    
    initial_state = {
        "messages": [],
        "domain": "hr",
        "taxonomy_context": dm.get_expert_context(),
        "secret_mission": mission,
        "is_concluded": False,
        "turn_count": 0,
        "max_turns": 8,
        "diagnosis_trace": [],
        "key_questions": [],
        "eliminated_categories": [],
        "confidence_history": [],
        "final_diagnosis": None
    }
    
    config = {"recursion_limit": 50}
    final_state = app.invoke(initial_state, config=config)
    
    print("-" * 60)
    print("ğŸ“Š ä»¿çœŸç»“æœ")
    print(f"   æ€»è½®æ¬¡: {final_state['turn_count']}")
    print(f"   ç½®ä¿¡åº¦æ›²çº¿: {final_state.get('confidence_history', [])}")
    print(f"   æ’é™¤åˆ†ç±»: {final_state.get('eliminated_categories', [])}")
    
    if final_state.get("final_diagnosis"):
        fd = final_state["final_diagnosis"]
        print(f"   æœ€ç»ˆè¯Šæ–­: {fd.get('service', 'N/A')}")
        print(f"   è¯Šæ–­ç½®ä¿¡åº¦: {fd.get('confidence', 0):.2%}")
    
    return final_state


if __name__ == "__main__":
    run_simulation_test()